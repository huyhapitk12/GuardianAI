# core/camera.py
# =============================================================================
# MODULE X·ª¨ L√ù CAMERA
# =============================================================================
# Module n√†y x·ª≠ l√Ω video t·ª´ camera v√† ch·∫°y c√°c b·ªô ph√°t hi·ªán:
# - Ph√°t hi·ªán ng∆∞·ªùi + nh·∫≠n di·ªán khu√¥n m·∫∑t
# - Ph√°t hi·ªán ch√°y/kh√≥i
# - Ph√¢n t√≠ch h√†nh vi b·∫•t th∆∞·ªùng
# =============================================================================

import cv2
import time
import queue
import platform
import threading
import numpy as np
from collections import deque

from config import settings, AlertType
from core.detection import PersonTracker, FireFilter, BehaviorAnalyzer, FireTracker
from core.motion_detector import MotionDetector


# =============================================================================
# CLASS CAMERA - X·ª¨ L√ù VIDEO T·ª™ M·ªòT CAMERA
# =============================================================================
class Camera:
    
    def __init__(self, source, person_alert_callback=None, fire_alert_callback=None, shared_model=None):
        """
        Kh·ªüi t·∫°o camera
        
        source: URL camera, ƒë∆∞·ªùng d·∫´n video, ho·∫∑c s·ªë (webcam ID)
        person_alert_callback: Callback khi ph√°t hi·ªán ng∆∞·ªùi
        fire_alert_callback: Callback khi ph√°t hi·ªán ch√°y
        shared_model: Model YOLO d√πng chung (ti·∫øt ki·ªám RAM)
        """
        # Ngu·ªìn video
        self.source = source
        self.source_id = str(source)
        
        # ƒê·ªëi t∆∞·ª£ng VideoCapture c·ªßa OpenCV
        self.cap = None
        
        # C·ªù b√°o hi·ªáu t·∫Øt
        self.quit = False
        
        # ----- Qu·∫£n l√Ω frame -----
        # Lock ƒë·ªÉ tr√°nh xung ƒë·ªôt khi nhi·ªÅu thread ƒë·ªçc/ghi frame
        self.frame_lock = threading.Lock()
        self.last_frame = None      # Frame ƒë√£ x·ª≠ l√Ω (c√≥ v·∫Ω box)
        self.raw_frame = None       # Frame g·ªëc (kh√¥ng v·∫Ω g√¨)
        self.frame_idx = 0          # ƒê·∫øm s·ªë frame
        
        # ----- Qu·∫£n l√Ω k·∫øt n·ªëi -----
        self.reconnect_attempts = 0
        self.last_frame_time = time.time()
        self.ai_active_until = 0    # Th·ªùi ƒëi·ªÉm AI t·∫Øt n·∫øu kh√¥ng c√≥ chuy·ªÉn ƒë·ªông
        
        # ----- Ph√°t hi·ªán ch·∫ø ƒë·ªô IR (h·ªìng ngo·∫°i/ban ƒë√™m) -----
        self.is_ir = False
        self.ir_history = deque(maxlen=30)  # L∆∞u l·ªãch s·ª≠ 30 frame
        
        # ----- Ph√°t hi·ªán ch√°y -----
        debug_fire = settings.get('camera.debug_fire_detection', False)
        self.fire_filter = FireFilter(debug=debug_fire)
        self.fire_boxes = []        # V·ªã tr√≠ c√°c ƒë√°m ch√°y
        self.fire_history = deque(maxlen=150)
        self.fire_tracker = FireTracker()
        
        # ----- Ph√°t hi·ªán ng∆∞·ªùi -----
        self.person_tracker = PersonTracker(shared_model=shared_model)
        
        # ----- Ph√¢n t√≠ch h√†nh vi -----
        self.behavior_analyzer = None
        self.last_pose = None           # L∆∞u pose cu·ªëi c√πng
        self.last_pose_time = 0         # Th·ªùi ƒëi·ªÉm pose cu·ªëi
        self.pose_hold_time = 0.3       # Gi·ªØ pose trong 0.3 gi√¢y ƒë·ªÉ tr√°nh nh·∫•p nh√°y
        
        # ----- Ph√°t hi·ªán chuy·ªÉn ƒë·ªông -----
        # D√πng ƒë·ªÉ ti·∫øt ki·ªám CPU: kh√¥ng c√≥ chuy·ªÉn ƒë·ªông = kh√¥ng c·∫ßn ch·∫°y AI
        self.motion_detector = MotionDetector(
            motion_threshold=settings.get('camera.motion_threshold', 25.0),
            min_area=settings.get('camera.motion_min_area', 500)
        )
        
        # ----- Callback functions -----
        self.person_alert_callback = person_alert_callback
        self.fire_alert_callback = fire_alert_callback
        
        # ----- Queue cho x·ª≠ l√Ω ƒëa lu·ªìng -----
        # maxsize=2: T·ªëi ƒëa 2 frame trong queue, tr√°nh t·ªìn ƒë·ªçng
        self.fire_queue = queue.Queue(maxsize=2)
        self.behavior_queue = queue.Queue(maxsize=2)
        self.result_queue = queue.Queue(maxsize=16)
        
        # Tr·∫°ng th√°i detection
        self.last_detection_enabled = False
        
        # K·∫øt n·ªëi camera
        self.init_capture()
    
    def init_capture(self):
        """K·∫øt n·ªëi v·ªõi camera"""
        try:
            # N·∫øu l√† webcam (s·ªë), th·ª≠ nhi·ªÅu backend
            if isinstance(self.source, int):
                backends = self.get_backends()
                for backend in backends:
                    self.cap = cv2.VideoCapture(self.source, backend)
                    if self.cap.isOpened():
                        break
            else:
                # URL ho·∫∑c file video
                self.cap = cv2.VideoCapture(self.source)
            
            # C·∫•u h√¨nh camera
            if self.cap and self.cap.isOpened():
                self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # Gi·∫£m ƒë·ªô tr·ªÖ
                print(f"‚úÖ Camera {self.source_id} ƒë√£ k·∫øt n·ªëi!")
                
        except Exception as e:
            print(f"‚ùå Camera {self.source_id} k·∫øt n·ªëi th·∫•t b·∫°i: {e}")
    
    def get_backends(self):
        """L·∫•y danh s√°ch backend ph√π h·ª£p v·ªõi h·ªá ƒëi·ªÅu h√†nh"""
        if platform.system() == 'Windows':
            return [cv2.CAP_MSMF, cv2.CAP_DSHOW, cv2.CAP_ANY]
        elif platform.system() == 'Linux':
            return [cv2.CAP_V4L2, cv2.CAP_ANY]
        return [cv2.CAP_ANY]
    
    def read(self):
        """ƒê·ªçc frame ƒë√£ x·ª≠ l√Ω (c√≥ v·∫Ω box, label)"""
        with self.frame_lock:
            if self.last_frame is not None:
                return True, self.last_frame.copy()
            return False, None
    
    def read_raw(self):
        """ƒê·ªçc frame g·ªëc (kh√¥ng x·ª≠ l√Ω)"""
        with self.frame_lock:
            if self.raw_frame is not None:
                return True, self.raw_frame.copy()
            return False, None
    
    def start_workers(self, fire_detector, face_detector, behavior_analyzer=None):
        """
        Kh·ªüi ƒë·ªông c√°c worker x·ª≠ l√Ω
        Worker = Thread ch·∫°y n·ªÅn ƒë·ªÉ x·ª≠ l√Ω t·ª´ng t√°c v·ª•
        """
        # G·∫Øn face detector v√†o person tracker
        self.person_tracker.set_face_detector(face_detector)
        self.person_tracker.initialize()
        
        # G·∫Øn behavior analyzer
        self.behavior_analyzer = behavior_analyzer
        
        # Thread ph√°t hi·ªán ch√°y
        threading.Thread(
            target=self.fire_worker,
            args=(fire_detector,),
            daemon=True
        ).start()
        
        # Thread ph√¢n t√≠ch h√†nh vi
        if self.behavior_analyzer:
            threading.Thread(
                target=self.behavior_worker,
                daemon=True
            ).start()
            print(f"‚úÖ Behavior worker ƒë√£ ch·∫°y cho camera {self.source_id}")
    
    def fire_worker(self, detector):
        """
        Worker ph√°t hi·ªán ch√°y
        Ch·∫°y trong thread ri√™ng, l·∫•y frame t·ª´ queue v√† ph√°t hi·ªán
        """
        while not self.quit:
            try:
                frame = self.fire_queue.get(timeout=1.0)
                detections = detector.detect(frame)
                if detections:
                    self.result_queue.put(('fire', detections))
            except queue.Empty:
                continue
    
    def behavior_worker(self):
        """
        Worker ph√¢n t√≠ch h√†nh vi
        Ph√°t hi·ªán h√†nh vi b·∫•t th∆∞·ªùng nh∆∞: ng√£, ƒë√°nh nhau,...
        """
        skip_counter = 0
        skip_n = settings.get('behavior.process_every_n_frames', 3)
        
        while not self.quit:
            try:
                frame = self.behavior_queue.get(timeout=1.0)
                
                # B·ªè qua m·ªôt s·ªë frame ƒë·ªÉ gi·∫£m t·∫£i
                skip_counter += 1
                if skip_counter % skip_n != 0:
                    continue
                
                # Ph√¢n t√≠ch
                result = self.behavior_analyzer.process_frame(frame)
                
                # Ki·ªÉm tra c√≥ b·∫•t th∆∞·ªùng kh√¥ng
                if result.is_anomaly and self.behavior_analyzer.should_alert():
                    self.result_queue.put(('behavior', result, frame.copy()))
                    
            except queue.Empty:
                continue
            except Exception as e:
                print(f"L·ªói behavior worker: {e}")
    
    # =========================================================================
    # V√íNG L·∫∂P X·ª¨ L√ù CH√çNH
    # =========================================================================
    def process_loop(self, state_manager):
        """
        V√≤ng l·∫∑p ch√≠nh x·ª≠ l√Ω video
        Ch·∫°y li√™n t·ª•c cho ƒë·∫øn khi self.quit = True
        """
        # T√≠nh interval gi·ªØa c√°c frame d·ª±a tr√™n FPS mong mu·ªën
        interval = 1.0 / settings.camera.target_fps
        last_time = 0
        cleanup_counter = 0
        
        while not self.quit:
            now = time.time()
            
            # ƒêi·ªÅu khi·ªÉn t·ªëc ƒë·ªô x·ª≠ l√Ω
            if now - last_time < interval:
                time.sleep(0.001)
                continue
            last_time = now
            
            # ----- Ki·ªÉm tra k·∫øt n·ªëi -----
            if not self.cap or not self.cap.isOpened():
                if not self.reconnect():
                    time.sleep(2.0)
                    continue
            
            # ----- ƒê·ªçc frame -----
            ret, frame = self.cap.read()
            if not ret or frame is None:
                if not self.check_health():
                    self.reconnect()
                continue
            
            self.last_frame_time = time.time()
            self.frame_idx += 1
            
            # ----- D·ªçn d·∫πp ƒë·ªãnh k·ª≥ -----
            cleanup_counter += 1
            if cleanup_counter >= 100:
                self.fire_filter.cleanup()
                cleanup_counter = 0
            
            # ----- Ph√°t hi·ªán ch·∫ø ƒë·ªô IR (m·ªói 10 frame) -----
            if self.frame_idx % 10 == 0:
                self.detect_ir(frame)
            
            # ----- √Åp d·ª•ng b·ªô l·ªçc m√†u -----
            frame = self.apply_color_filter(frame)
            
            # ----- Resize frame ƒë·ªÉ x·ª≠ l√Ω -----
            # Frame nh·ªè h∆°n = x·ª≠ l√Ω nhanh h∆°n
            proc_size = settings.camera.process_size
            small = cv2.resize(frame, tuple(proc_size))
            
            # T√≠nh t·ªâ l·ªá scale ƒë·ªÉ chuy·ªÉn ƒë·ªïi t·ªça ƒë·ªô
            h, w = frame.shape[:2]
            scale_x = w / proc_size[0]
            scale_y = h / proc_size[1]
            
            # ----- Ki·ªÉm tra detection c√≥ b·∫≠t kh√¥ng -----
            detection_enabled = state_manager.is_detection_enabled(self.source_id)
            self.last_detection_enabled = detection_enabled
            
            # ----- Ph√°t hi·ªán chuy·ªÉn ƒë·ªông -----
            has_motion = self.motion_detector.detect(small)
            
            # ===== LOGIC TH√îNG MINH: Ti·∫øt ki·ªám CPU =====
            # 1. C√≥ chuy·ªÉn ƒë·ªông ‚Üí B·∫≠t AI 5 gi√¢y
            if has_motion:
                self.ai_active_until = now + 5.0
            
            # 2. Ch·ªâ ch·∫°y AI khi c·∫ßn
            should_run_ai = detection_enabled and (
                now < self.ai_active_until or self.frame_idx < 30
            )
            
            if should_run_ai:
                self.process_persons(small, frame, scale_x, scale_y)
                
                # 3. N·∫øu c√≥ ng∆∞·ªùi, gi·ªØ AI ho·∫°t ƒë·ªông (tr√°nh m·∫•t track khi ƒë·ª©ng y√™n)
                if self.person_tracker.has_tracks():
                    self.ai_active_until = now + 5.0
            
            # ----- Ph√°t hi·ªán ch√°y (lu√¥n ch·∫°y v√¨ quan tr·ªçng) -----
            if not self.fire_queue.full():
                self.fire_queue.put(small.copy())
                self.fire_queue.put(small.copy())
            
            # ----- Ph√¢n t√≠ch h√†nh vi -----
            # [LOGIC] Ch·ªâ ch·∫°y AI h√†nh vi khi ƒê√É ph√°t hi·ªán ng∆∞·ªùi
            has_people = self.person_tracker.has_tracks()
            if detection_enabled and self.behavior_analyzer and not self.behavior_queue.full() and has_people:
                self.behavior_queue.put(small.copy())
            
            # ----- X·ª≠ l√Ω k·∫øt qu·∫£ t·ª´ c√°c worker -----
            self.process_results(frame, scale_x, scale_y)
            
            # ----- C·∫≠p nh·∫≠t frame hi·ªÉn th·ªã -----
            display = frame.copy()
            self.draw_overlays(display, detection_enabled)
            
            with self.frame_lock:
                self.last_frame = display
                self.raw_frame = frame.copy()
        
        # D·ªçn d·∫πp khi tho√°t
        self.release()
    
    def process_persons(self, small, full, scale_x, scale_y):
        """X·ª≠ l√Ω ph√°t hi·ªán v√† tracking ng∆∞·ªùi"""
        try:
            # L·∫•y ng∆∞·ª°ng tin c·∫≠y
            threshold = settings.get('detection.person_confidence', 0.5)
            if self.is_ir:
                # IR mode: ng∆∞·ª°ng th·∫•p h∆°n v√¨ ·∫£nh kh√≥ h∆°n
                threshold = settings.get('camera.infrared.person_detection_threshold', 0.45)
            
            # Ph√°t hi·ªán ng∆∞·ªùi
            detections = self.person_tracker.detect(small, threshold)
            
            # C·∫≠p nh·∫≠t tracking
            if self.is_ir:
                # IR: B·ªè qua nh·∫≠n di·ªán khu√¥n m·∫∑t (kh√¥ng c√≥ m√†u)
                self.person_tracker.update(detections, full, scale_x, scale_y, skip_face_check=True)
            else:
                self.person_tracker.update(detections, full, scale_x, scale_y)
            
            # Ki·ªÉm tra c·∫£nh b√°o
            for tid, alert_type, metadata in self.person_tracker.check_alerts():
                if self.person_alert_callback:
                    alert_frame = full.copy()
                    self.draw_overlays(alert_frame, True)
                    self.person_alert_callback(self.source_id, alert_frame, alert_type, metadata)
                    
        except Exception as e:
            print(f"L·ªói x·ª≠ l√Ω ng∆∞·ªùi: {e}")
    
    def process_results(self, frame, scale_x, scale_y):
        """X·ª≠ l√Ω k·∫øt qu·∫£ t·ª´ c√°c worker queue"""
        self.fire_boxes = []
        
        try:
            while not self.result_queue.empty():
                result = self.result_queue.get_nowait()
                result_type = result[0]
                
                if result_type == 'fire':
                    detections = result[1]
                    self.handle_fire_detections(detections, frame, scale_x, scale_y)
                
                elif result_type == 'behavior':
                    behavior_result = result[1]
                    alert_frame = result[2]
                    self.handle_behavior_alert(behavior_result, alert_frame)
                    
        except queue.Empty:
            pass
    
    def handle_fire_detections(self, detections, frame, scale_x, scale_y):
        """
        X·ª≠ l√Ω ph√°t hi·ªán ch√°y v·ªõi h·ªá th·ªëng Red Alert Mode
        
        Yellow Alert: Nghi ng·ªù c√≥ ch√°y (c·∫ßn x√°c nh·∫≠n th√™m)
        Red Alert: Ch·∫Øc ch·∫Øn c√≥ ch√°y (nguy hi·ªÉm!)
        """
        validated_dets = []
        
        for det in detections:
            bbox = det['bbox']
            
            # Validate v·ªõi b·ªô l·ªçc (lo·∫°i b·ªè false positive)
            if not self.fire_filter.validate(frame, bbox, self.is_ir):
                continue
            
            # Scale t·ªça ƒë·ªô v·ªÅ k√≠ch th∆∞·ªõc g·ªëc
            x1, y1, x2, y2 = bbox
            scaled_bbox = (
                int(x1 * scale_x), int(y1 * scale_y),
                int(x2 * scale_x), int(y2 * scale_y)
            )
            
            self.fire_boxes.append(scaled_bbox)
            self.fire_history.append({'time': time.time(), **det})
            validated_dets.append(det)
        
        # C·∫≠p nh·∫≠t fire tracker v√† ki·ªÉm tra ƒëi·ªÅu ki·ªán c·∫£nh b√°o
        now = time.time()
        should_alert, is_yellow, is_red = self.fire_tracker.update(validated_dets, now)
        
        # G·ª≠i c·∫£nh b√°o n·∫øu c·∫ßn
        if should_alert and self.fire_alert_callback:
            alert_frame = frame.copy()
            self.draw_overlays(alert_frame, True)
            
            # Red = CRITICAL, Yellow = WARNING
            alert_type = AlertType.FIRE_CRITICAL if is_red else AlertType.FIRE_WARNING
            
            if is_red:
                print(f"üî¥ RED ALERT - Camera {self.source_id}")
            elif is_yellow:
                print(f"üü° Yellow Alert - Camera {self.source_id}")
            
            self.fire_alert_callback(self.source_id, alert_frame, alert_type)
    
    def handle_behavior_alert(self, result, frame):
        """X·ª≠ l√Ω c·∫£nh b√°o h√†nh vi b·∫•t th∆∞·ªùng"""
        if self.person_alert_callback:
            # V·∫Ω visualization
            if self.behavior_analyzer:
                self.behavior_analyzer.draw_on_frame(frame, result)
            
            metadata = {
                'score': result.score,
                'timestamp': result.timestamp
            }
            self.person_alert_callback(
                self.source_id,
                frame,
                AlertType.ANOMALOUS_BEHAVIOR,
                metadata
            )
    
    def draw_overlays(self, frame, detection_enabled):
        """V·∫Ω c√°c th√¥ng tin l√™n frame"""
        
        # ----- V·∫Ω box ch√°y (ƒë·ªè) -----
        for box in self.fire_boxes:
            cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 0, 255), 3)
            cv2.putText(frame, "üî• FIRE", (box[0], box[1] - 5),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        # ----- V·∫Ω box ng∆∞·ªùi -----
        if detection_enabled:
            self.draw_persons_with_behavior(frame)
        
        # ----- V·∫Ω box chuy·ªÉn ƒë·ªông (Cyan) -----
        if hasattr(self.motion_detector, 'motion_boxes'):
            dh, dw = frame.shape[:2]
            ph, pw = settings.camera.process_size[1], settings.camera.process_size[0]
            sx = dw / pw
            sy = dh / ph
            
            for (mx1, my1, mx2, my2) in self.motion_detector.motion_boxes:
                final_x1 = int(mx1 * sx)
                final_y1 = int(my1 * sy)
                final_x2 = int(mx2 * sx)
                final_y2 = int(my2 * sy)
                
                cv2.rectangle(frame, (final_x1, final_y1), (final_x2, final_y2), (255, 255, 0), 1)
                cv2.putText(frame, "Motion", (final_x1, final_y1 - 2), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 0), 1)
        
        # ----- Hi·ªÉn th·ªã ch·∫ø ƒë·ªô IR -----
        if self.is_ir:
            cv2.putText(frame, "IR MODE", (10, frame.shape[0] - 20),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
    
    def draw_persons_with_behavior(self, frame):
        """V·∫Ω box ng∆∞·ªùi k√®m tr·∫°ng th√°i h√†nh vi"""
        tracks = self.person_tracker.tracks
        
        # L·∫•y behavior score
        behavior_score = 0.0
        behavior_threshold = 0.5
        
        if self.behavior_analyzer:
            behavior_score = self.behavior_analyzer.current_score
            behavior_threshold = self.behavior_analyzer.threshold
        
        is_anomaly = behavior_score >= behavior_threshold
        
        for tid, track in tracks.items():
            x1, y1, x2, y2 = map(int, track.bbox)
            
            # X√°c ƒë·ªãnh t√™n hi·ªÉn th·ªã
            name = track.confirmed_name or track.name
            is_stranger = (name == "Stranger")
            
            # ===== X√ÅC ƒê·ªäNH M√ÄU BOX =====
            if is_anomaly:
                color = (0, 0, 255)      # ƒê·ªè - B·∫•t th∆∞·ªùng
                status = "BAT THUONG"
            elif is_stranger:
                color = (0, 165, 255)    # Cam - Ng∆∞·ªùi l·∫°
                status = "Chua xac dinh"
            else:
                color = (0, 255, 0)      # Xanh l√° - Ng∆∞·ªùi quen
                status = "Binh thuong"
            
            # ===== V·∫º BOX =====
            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
            
            # ===== T·∫†O LABEL =====
            if self.behavior_analyzer and self.behavior_analyzer.loaded:
                label = f"ID:{tid} {name} | {status} ({behavior_score:.2f})"
            else:
                label = f"ID:{tid} {name}"
            
            # ===== V·∫º LABEL =====
            font = cv2.FONT_HERSHEY_SIMPLEX
            font_scale = 0.5
            thickness = 1
            (text_w, text_h), _ = cv2.getTextSize(label, font, font_scale, thickness)
            
            label_y1 = max(0, y1 - text_h - 10)
            label_y2 = y1 - 2
            label_x2 = min(frame.shape[1], x1 + text_w + 8)
            
            cv2.rectangle(frame, (x1, label_y1), (label_x2, label_y2), color, -1)
            cv2.putText(frame, label, (x1 + 4, label_y2 - 4), font, font_scale, (255, 255, 255), thickness)
        
        # ===== V·∫º SKELETON =====
        # L·∫•y pose hi·ªán t·∫°i t·ª´ analyzer
        current_pose = self.behavior_analyzer.current_pose if self.behavior_analyzer else None
        now = time.time()
        
        # C·∫≠p nh·∫≠t last_pose n·∫øu c√≥ pose m·ªõi
        if current_pose and current_pose.is_valid:
            self.last_pose = current_pose
            self.last_pose_time = now
        
        # V·∫Ω skeleton n·∫øu c√≥ pose v√† ch∆∞a qu√° th·ªùi gian hold
        if self.last_pose and self.last_pose.bbox and (now - self.last_pose_time < self.pose_hold_time):
            # [LOGIC M·ªöI] Ch·ªâ v·∫Ω n·∫øu skeleton n·∫±m trong v√πng c·ªßa ng∆∞·ªùi ƒë√£ ph√°t hi·ªán
            # ƒêi·ªÅu n√†y gi√∫p ƒë·ªìng b·ªô gi·ªØa Person Detection v√† Behavior Analysis
            should_draw = False
            
            # 1. T√≠nh to√°n t·ªça ƒë·ªô skeleton tr√™n frame hi·ªÉn th·ªã
            h, w = frame.shape[:2]
            proc_w, proc_h = settings.camera.process_size
            scale_x = w / proc_w
            scale_y = h / proc_h
            
            px1, py1, px2, py2 = self.last_pose.bbox
            # Box c·ªßa skeleton (ƒë√£ scale)
            sk_x1 = px1 * scale_x
            sk_y1 = py1 * scale_y
            sk_x2 = px2 * scale_x
            sk_y2 = py2 * scale_y
            
            # T√¢m c·ªßa skeleton
            sk_cx = (sk_x1 + sk_x2) / 2
            sk_cy = (sk_y1 + sk_y2) / 2
            
            # 2. Ki·ªÉm tra c√≥ tr√πng v·ªõi ng∆∞·ªùi n√†o kh√¥ng
            for tid, track in tracks.items():
                tx1, ty1, tx2, ty2 = track.bbox
                
                # Ki·ªÉm tra t√¢m skeleton n·∫±m trong box ng∆∞·ªùi
                # M·ªü r·ªông box ng∆∞·ªùi m·ªôt ch√∫t (margin) ƒë·ªÉ b·∫Øt d√≠nh t·ªët h∆°n
                margin = 50 
                if (tx1 - margin <= sk_cx <= tx2 + margin) and \
                   (ty1 - margin <= sk_cy <= ty2 + margin):
                    should_draw = True
                    break
            
            if should_draw:
                self.draw_skeleton_only(frame, is_anomaly, self.last_pose)

    def draw_skeleton_only(self, frame, is_anomaly, pose):
        """
        V·∫Ω skeleton (b·ªô x∆∞∆°ng) c·ªßa ng∆∞·ªùi
        
        pose: PoseResult ch·ª©a keypoints ƒë√£ ·ªü t·ªça ƒë·ªô process_size
        """
        if not pose or not pose.is_valid:
            return
        
        # Scale keypoints t·ª´ process_size v·ªÅ k√≠ch th∆∞·ªõc frame hi·ªÉn th·ªã
        h, w = frame.shape[:2]
        proc_w, proc_h = settings.camera.process_size
        
        scale_x = w / proc_w
        scale_y = h / proc_h
        
        # Copy v√† scale keypoints
        scaled_kps = pose.keypoints.copy()
        scaled_kps[:, 0] *= scale_x
        scaled_kps[:, 1] *= scale_y
        
        # M√†u theo tr·∫°ng th√°i
        color = (0, 0, 255) if is_anomaly else (0, 255, 0)
        
        # C√°c ƒë∆∞·ªùng n·ªëi skeleton (theo format COCO)
        SKELETON = [
            (0, 1), (0, 2), (1, 3), (2, 4),      # ƒê·∫ßu
            (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),  # Tay
            (5, 11), (6, 12), (11, 12),          # Th√¢n
            (11, 13), (13, 15), (12, 14), (14, 16)   # Ch√¢n
        ]
        
        # V·∫Ω x∆∞∆°ng
        for i, j in SKELETON:
            if i < len(pose.confidence) and j < len(pose.confidence):
                if pose.confidence[i] > 0.3 and pose.confidence[j] > 0.3:
                    pt1 = tuple(scaled_kps[i].astype(int))
                    pt2 = tuple(scaled_kps[j].astype(int))
                    cv2.line(frame, pt1, pt2, color, 2)
        
        # V·∫Ω kh·ªõp
        for pt, conf in zip(scaled_kps, pose.confidence):
            if conf > 0.3:
                center = tuple(pt.astype(int))
                cv2.circle(frame, center, 5, color, -1)
                cv2.circle(frame, center, 5, (255, 255, 255), 1)
    
    def detect_ir(self, frame):
        """
        Ph√°t hi·ªán ch·∫ø ƒë·ªô IR (h·ªìng ngo·∫°i/ban ƒë√™m)
        
        Camera IR ch·ªâ c√≥ ƒëen tr·∫Øng, kh√¥ng c√≥ m√†u.
        Khi camera chuy·ªÉn sang ban ƒë√™m, c·∫ßn ƒëi·ªÅu ch·ªânh c√°c ng∆∞·ª°ng.
        """
        # L·∫•y m·∫´u (sample) ƒë·ªÉ t√≠nh nhanh
        sample = frame[::10, ::10]
        
        # T√°ch k√™nh m√†u
        b, g, r = cv2.split(sample.astype(np.float32))
        
        # T√≠nh trung b√¨nh v√† ƒë·ªô l·ªách chu·∫©n
        means = [np.mean(r), np.mean(g), np.mean(b)]
        std = np.std(means)
        ratio = min(means) / max(means) if max(means) > 0 else 1.0
        
        # T√≠nh ƒë·ªô b√£o h√≤a
        hsv = cv2.cvtColor(sample.astype(np.uint8), cv2.COLOR_BGR2HSV)
        sat = np.mean(hsv[:, :, 1])
        
        # IR: C√°c k√™nh m√†u g·∫ßn b·∫±ng nhau + ƒë·ªô b√£o h√≤a th·∫•p
        is_ir = std < 2.0 and ratio > 0.98 and sat < 10
        self.ir_history.append(is_ir)
        
        # C·∫ßn ƒë·ªß l·ªãch s·ª≠ ƒë·ªÉ quy·∫øt ƒë·ªãnh
        if len(self.ir_history) >= 10:
            ir_ratio = sum(self.ir_history) / len(self.ir_history)
            new_mode = ir_ratio >= 0.7
            
            # Th√¥ng b√°o khi chuy·ªÉn ch·∫ø ƒë·ªô
            if new_mode != self.is_ir:
                self.is_ir = new_mode
                mode_name = 'IR (Ban ƒë√™m)' if new_mode else 'RGB (Ban ng√†y)'
                print(f"üì∑ Camera {self.source_id}: Chuy·ªÉn sang ch·∫ø ƒë·ªô {mode_name}")
                if new_mode:
                    print(f"   ‚Üí T·∫Øt nh·∫≠n di·ªán khu√¥n m·∫∑t (·∫£nh ƒëen tr·∫Øng)")
    
    def apply_color_filter(self, frame):
        """√Åp d·ª•ng b·ªô l·ªçc m√†u theo ch·∫ø ƒë·ªô"""
        if self.is_ir:
            # Chuy·ªÉn sang grayscale ƒë·ªÉ x·ª≠ l√Ω th·ªëng nh·∫•t
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            return cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)
        
        # Tr·∫£ v·ªÅ frame g·ªëc
        return frame
    
    def reconnect(self):
        """Th·ª≠ k·∫øt n·ªëi l·∫°i camera"""
        self.reconnect_attempts += 1
        
        max_attempts = settings.get('camera.max_reconnect_attempts', 10)
        if self.reconnect_attempts > max_attempts:
            print(f"‚ùå Camera {self.source_id}: ƒê√£ th·ª≠ {max_attempts} l·∫ßn, d·ª´ng k·∫øt n·ªëi l·∫°i")
            return False
        
        print(f"ƒêang k·∫øt n·ªëi l·∫°i camera {self.source_id}... (l·∫ßn {self.reconnect_attempts}/{max_attempts})")
        
        if self.cap:
            self.cap.release()
        
        time.sleep(2.0)
        self.init_capture()
        
        if self.cap and self.cap.isOpened():
            self.reconnect_attempts = 0
            return True
        
        return False
    
    def check_health(self):
        """Ki·ªÉm tra camera c√≤n ho·∫°t ƒë·ªông kh√¥ng"""
        return time.time() - self.last_frame_time < 10
    
    def get_connection_status(self):
        """L·∫•y tr·∫°ng th√°i k·∫øt n·ªëi"""
        return self.cap is not None and self.cap.isOpened() and self.check_health()
    
    def has_active_threat(self):
        """
        Ki·ªÉm tra c√≥ m·ªëi nguy hi·ªÉm ƒëang ho·∫°t ƒë·ªông kh√¥ng
        D√πng ƒë·ªÉ quy·∫øt ƒë·ªãnh c√≥ k√©o d√†i th·ªùi gian ghi video kh√¥ng
        """
        # 1. Ki·ªÉm tra ch√°y
        if self.fire_tracker.is_red_alert or self.fire_tracker.is_yellow_alert:
            return True
        
        # 2. Ki·ªÉm tra ng∆∞·ªùi l·∫°
        if self.person_tracker.has_active_threats():
            return True
        
        # 3. Ki·ªÉm tra h√†nh vi b·∫•t th∆∞·ªùng
        if self.behavior_analyzer:
            if self.behavior_analyzer.current_score >= self.behavior_analyzer.threshold:
                return True
        
        return False
    
    def get_infrared_status(self):
        """L·∫•y tr·∫°ng th√°i IR"""
        return self.is_ir
    
    def force_reconnect(self):
        """B·∫Øt bu·ªôc k·∫øt n·ªëi l·∫°i"""
        self.reconnect_attempts = 0
        self.reconnect()
    
    def release(self):
        """Gi·∫£i ph√≥ng t√†i nguy√™n"""
        self.quit = True
        if self.cap:
            self.cap.release()
            self.cap = None
        if self.behavior_analyzer:
            self.behavior_analyzer.close()
            self.behavior_analyzer = None
        print(f"Camera {self.source_id} ƒë√£ gi·∫£i ph√≥ng")
