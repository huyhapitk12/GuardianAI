# core/camera.py
# =============================================================================
# MODULE X·ª¨ L√ù CAMERA
# =============================================================================
# Module n√†y x·ª≠ l√Ω video t·ª´ camera v√† ch·∫°y c√°c b·ªô ph√°t hi·ªán:
# - Ph√°t hi·ªán ng∆∞·ªùi + nh·∫≠n di·ªán khu√¥n m·∫∑t
# - Ph√°t hi·ªán ch√°y/kh√≥i
# =============================================================================

import cv2
import time
import queue
import platform
import threading
import numpy as np
from collections import deque

from config import settings, AlertType
from core.detection import PersonTracker, FireFilter, FireTracker, FallDetector
from core.motion_detector import MotionDetector


# =============================================================================
# CLASS CAMERA - X·ª¨ L√ù VIDEO T·ª™ M·ªòT CAMERA
# =============================================================================
class Camera:
    
    # Kh·ªüi t·∫°o camera
    # source: URL camera, ƒë∆∞·ªùng d·∫´n video, ho·∫∑c s·ªë (webcam ID)
    # person_alert_callback: Callback khi ph√°t hi·ªán ng∆∞·ªùi
    # fire_alert_callback: Callback khi ph√°t hi·ªán ch√°y
    # fall_alert_callback: Callback khi ph√°t hi·ªán t√© ng√£
    # shared_model: Model YOLO d√πng chung (ti·∫øt ki·ªám RAM)
    def __init__(self, source, person_alert_callback=None, fire_alert_callback=None, fall_alert_callback=None, shared_model=None):
        # Ngu·ªìn video
        self.source = source
        self.source_id = str(source)
        
        # ƒê·ªëi t∆∞·ª£ng VideoCapture c·ªßa OpenCV
        self.cap = None
        
        # C·ªù b√°o hi·ªáu t·∫Øt
        self.quit = False
        
        # ----- Qu·∫£n l√Ω frame -----
        # Lock ƒë·ªÉ tr√°nh xung ƒë·ªôt khi nhi·ªÅu thread ƒë·ªçc/ghi frame
        self.frame_lock = threading.Lock()
        self.last_frame = None      # Frame ƒë√£ x·ª≠ l√Ω (c√≥ v·∫Ω box)
        self.raw_frame = None       # Frame g·ªëc (kh√¥ng v·∫Ω g√¨)
        self.frame_idx = 0          # ƒê·∫øm s·ªë frame
        
        # ----- Qu·∫£n l√Ω k·∫øt n·ªëi -----
        self.reconnect_attempts = 0
        self.last_frame_time = time.time()
        self.ai_active_until = 0    # Th·ªùi ƒëi·ªÉm AI t·∫Øt n·∫øu kh√¥ng c√≥ chuy·ªÉn ƒë·ªông
        
        # ----- Ph√°t hi·ªán ch·∫ø ƒë·ªô IR (h·ªìng ngo·∫°i/ban ƒë√™m) -----
        self.is_ir = False
        self.ir_history = deque(maxlen=30)  # L∆∞u l·ªãch s·ª≠ 30 frame
        self.ir_manual_override = None  # None = auto, True/False = manual
        
        # ----- Ph√°t hi·ªán ch√°y -----
        debug_fire = settings.get('camera.debug_fire_detection', False)
        self.fire_filter = FireFilter(debug=debug_fire)
        self.fire_boxes = []        # V·ªã tr√≠ c√°c ƒë√°m ch√°y
        self.fire_history = deque(maxlen=150)
        self.fire_tracker = FireTracker()
        
        # ----- Ph√°t hi·ªán ng∆∞·ªùi -----
        self.person_tracker = PersonTracker(shared_model=shared_model)
        
        # ----- Ph√°t hi·ªán chuy·ªÉn ƒë·ªông -----
        # D√πng ƒë·ªÉ ti·∫øt ki·ªám CPU: kh√¥ng c√≥ chuy·ªÉn ƒë·ªông = kh√¥ng c·∫ßn ch·∫°y AI
        self.motion_detector = MotionDetector(
            motion_threshold=settings.get('camera.motion_threshold', 25.0),
            min_area=settings.get('camera.motion_min_area', 500)
        )
        
        # ----- Callback functions -----
        self.person_alert_callback = person_alert_callback
        self.fire_alert_callback = fire_alert_callback
        self.fall_alert_callback = fall_alert_callback
        
        # ----- Ph√°t hi·ªán t√© ng√£ -----
        self.fall_detector = None
        self.is_fall_detected = False
        self.fall_prob = 0.0
        
        # ----- Queue cho x·ª≠ l√Ω ƒëa lu·ªìng -----
        # maxsize=2: T·ªëi ƒëa 2 frame trong queue, tr√°nh t·ªìn ƒë·ªçng
        self.fire_queue = queue.Queue(maxsize=2)
        self.result_queue = queue.Queue(maxsize=16)
        
        # Tr·∫°ng th√°i detection
        self.last_detection_enabled = False
        
        # Override per-camera settings (None = d√πng global settings)
        self.face_enabled = None
        
        # K·∫øt n·ªëi camera
        self.init_capture()
    
    # K·∫øt n·ªëi v·ªõi camera
    def init_capture(self):
        try:
            # N·∫øu l√† webcam (s·ªë), th·ª≠ nhi·ªÅu backend
            if isinstance(self.source, int):
                backends = self.get_backends()
                for backend in backends:
                    self.cap = cv2.VideoCapture(self.source, backend)
                    if self.cap.isOpened():
                        break
            else:
                # URL ho·∫∑c file video
                self.cap = cv2.VideoCapture(self.source)
            
            # C·∫•u h√¨nh camera
            if self.cap and self.cap.isOpened():
                self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # Gi·∫£m ƒë·ªô tr·ªÖ
                print(f"‚úÖ Camera {self.source_id} ƒë√£ k·∫øt n·ªëi!")
                
        except Exception as e:
            print(f"‚ùå Camera {self.source_id} k·∫øt n·ªëi th·∫•t b·∫°i: {e}")
    
    # L·∫•y danh s√°ch backend ph√π h·ª£p v·ªõi h·ªá ƒëi·ªÅu h√†nh
    def get_backends(self):
        if platform.system() == 'Windows':
            return [cv2.CAP_MSMF, cv2.CAP_DSHOW, cv2.CAP_ANY]
        elif platform.system() == 'Linux':
            return [cv2.CAP_V4L2, cv2.CAP_ANY]
        return [cv2.CAP_ANY]
    
    # ƒê·ªçc frame ƒë√£ x·ª≠ l√Ω (c√≥ v·∫Ω box, label)
    def read(self):
        with self.frame_lock:
            if self.last_frame is not None:
                return True, self.last_frame.copy()
            return False, None
    
    # ƒê·ªçc frame g·ªëc (kh√¥ng x·ª≠ l√Ω)
    def read_raw(self):
        with self.frame_lock:
            if self.raw_frame is not None:
                return True, self.raw_frame.copy()
            return False, None
    
    # Kh·ªüi ƒë·ªông c√°c worker x·ª≠ l√Ω
    # Worker = Thread ch·∫°y n·ªÅn ƒë·ªÉ x·ª≠ l√Ω t·ª´ng t√°c v·ª•
    def start_workers(self, fire_detector, face_detector):
        # G·∫Øn face detector v√†o person tracker
        self.person_tracker.set_face_detector(face_detector)
        self.person_tracker.initialize()
        
        # Kh·ªüi t·∫°o Fall Detector
        try:
            self.fall_detector = FallDetector()
            print(f"‚úÖ Camera {self.source_id}: Fall Detector ƒë√£ s·∫µn s√†ng")
        except Exception as e:
            print(f"‚ö†Ô∏è Camera {self.source_id}: Kh√¥ng th·ªÉ kh·ªüi t·∫°o Fall Detector: {e}")
            self.fall_detector = None
        
        # Thread ph√°t hi·ªán ch√°y
        threading.Thread(
            target=self.fire_worker,
            args=(fire_detector,),
            daemon=True
        ).start()
    
    # Worker ph√°t hi·ªán ch√°y
    # Ch·∫°y trong thread ri√™ng, l·∫•y frame t·ª´ queue v√† ph√°t hi·ªán
    def fire_worker(self, detector):
        while not self.quit:
            try:
                frame = self.fire_queue.get(timeout=1.0)
                detections = detector.detect(frame)
                if detections:
                    self.result_queue.put(('fire', detections))
            except queue.Empty:
                continue
    
    # =========================================================================
    # V√íNG L·∫∂P X·ª¨ L√ù CH√çNH
    # =========================================================================
    # V√≤ng l·∫∑p ch√≠nh x·ª≠ l√Ω video
    # Ch·∫°y li√™n t·ª•c cho ƒë·∫øn khi self.quit = True
    def process_loop(self, state_manager):
        # T√≠nh interval gi·ªØa c√°c frame d·ª±a tr√™n FPS mong mu·ªën
        interval = 1.0 / settings.camera.target_fps
        last_time = 0
        cleanup_counter = 0
        
        while not self.quit:
            now = time.time()
            
            # ƒêi·ªÅu khi·ªÉn t·ªëc ƒë·ªô x·ª≠ l√Ω
            if now - last_time < interval:
                time.sleep(0.001)
                continue
            last_time = now
            
            # ----- Ki·ªÉm tra k·∫øt n·ªëi -----
            if not self.cap or not self.cap.isOpened():
                if not self.reconnect():
                    time.sleep(2.0)
                    continue
            
            # ----- ƒê·ªçc frame -----
            ret, frame = self.cap.read()
            if not ret or frame is None:
                if not self.check_health():
                    self.reconnect()
                continue
            
            self.last_frame_time = time.time()
            self.frame_idx += 1
            
            # ----- D·ªçn d·∫πp ƒë·ªãnh k·ª≥ -----
            cleanup_counter += 1
            if cleanup_counter >= 100:
                self.fire_filter.cleanup()
                cleanup_counter = 0
            
            # ----- Ph√°t hi·ªán ch·∫ø ƒë·ªô IR (m·ªói 10 frame) -----
            if self.frame_idx % 10 == 0:
                self.detect_ir(frame)
            
            # ----- √Åp d·ª•ng b·ªô l·ªçc m√†u -----
            frame = self.apply_color_filter(frame)
            
            # ----- Resize frame ƒë·ªÉ x·ª≠ l√Ω -----
            # Frame nh·ªè h∆°n = x·ª≠ l√Ω nhanh h∆°n
            # [LOGIC] Gi·ªØ nguy√™n t·ªâ l·ªá (aspect ratio) ƒë·ªÉ kh√¥ng b·ªã m√©o
            proc_w, proc_h = settings.camera.process_size
            h, w = frame.shape[:2]
            
            # T√≠nh scale factor ƒë·ªÉ fit v√†o process_size
            scale = min(proc_w / w, proc_h / h)
            new_w = int(w * scale)
            new_h = int(h * scale)
            
            small = cv2.resize(frame, (new_w, new_h))
            
            # T√≠nh t·ªâ l·ªá scale ƒë·ªÉ chuy·ªÉn ƒë·ªïi t·ªça ƒë·ªô (t·ª´ nh·ªè -> l·ªõn)
            scale_x = w / new_w
            scale_y = h / new_h
            
            # ----- Ki·ªÉm tra detection c√≥ b·∫≠t kh√¥ng -----
            detection_enabled = state_manager.is_detection_enabled(self.source_id)
            self.last_detection_enabled = detection_enabled
            
            # ----- Ph√°t hi·ªán chuy·ªÉn ƒë·ªông -----
            has_motion = self.motion_detector.detect(small)
            
            # ===== LOGIC TH√îNG MINH: Ti·∫øt ki·ªám CPU =====
            # 1. C√≥ chuy·ªÉn ƒë·ªông ‚Üí B·∫≠t AI 5 gi√¢y
            if has_motion:
                self.ai_active_until = now + 5.0
            
            # 2. Ch·ªâ ch·∫°y AI khi c·∫ßn
            # ----- Ki·ªÉm tra toggle ch·ª©c nƒÉng -----
            # ∆Øu ti√™n setting ri√™ng c·ªßa camera, n·∫øu None th√¨ d√πng Global
            face_enabled = self.face_enabled if self.face_enabled is not None else settings.get('detection.face_recognition_enabled', True)
            
            # Ch·ªâ ch·∫°y AI khi face detection ƒë∆∞·ª£c b·∫≠t
            should_run_ai = detection_enabled and face_enabled and (
                now < self.ai_active_until or self.frame_idx < 30
            )
            
            if should_run_ai:
                self.process_persons(small, frame, scale_x, scale_y, face_enabled)
                
                # 3. N·∫øu c√≥ ng∆∞·ªùi, gi·ªØ AI ho·∫°t ƒë·ªông (tr√°nh m·∫•t track khi ƒë·ª©ng y√™n)
                if self.person_tracker.has_tracks():
                    self.ai_active_until = now + 5.0
                    
                    # 4. Ph√°t hi·ªán t√© ng√£ (ch·ªâ khi c√≥ ng∆∞·ªùi)
                    self.process_fall(frame, scale_x, scale_y)
            
            # ----- Ph√°t hi·ªán ch√°y (lu√¥n ch·∫°y v√¨ quan tr·ªçng) -----
            if not self.fire_queue.full():
                self.fire_queue.put(small.copy())
                self.fire_queue.put(small.copy())
            
            # ----- X·ª≠ l√Ω k·∫øt qu·∫£ t·ª´ c√°c worker -----
            self.process_results(frame, scale_x, scale_y)
            
            # ----- C·∫≠p nh·∫≠t frame hi·ªÉn th·ªã -----
            display = frame.copy()
            self.draw_overlays(display, detection_enabled, scale_x, scale_y)
            
            with self.frame_lock:
                self.last_frame = display
                self.raw_frame = frame.copy()
        
        # D·ªçn d·∫πp khi tho√°t
        self.release()
    
    # X·ª≠ l√Ω ph√°t hi·ªán v√† tracking ng∆∞·ªùi
    def process_persons(self, small, full, scale_x, scale_y, face_enabled=True):
        try:
            # L·∫•y ng∆∞·ª°ng tin c·∫≠y
            threshold = settings.get('detection.person_confidence_threshold', 0.5)
            if self.is_ir:
                # IR mode: ng∆∞·ª°ng th·∫•p h∆°n v√¨ ·∫£nh kh√≥ h∆°n
                threshold = settings.get('camera.infrared.person_detection_threshold', 0.45)
            
            # Ph√°t hi·ªán ng∆∞·ªùi
            detections = self.person_tracker.detect(small, threshold)
            
            # C·∫≠p nh·∫≠t tracking
            # Skip face check n·∫øu: IR mode HO·∫∂C Face Recognition b·ªã t·∫Øt
            skip_face = self.is_ir or not face_enabled
            
            self.person_tracker.update(detections, full, scale_x, scale_y, skip_face_check=skip_face)
            
            # Ki·ªÉm tra c·∫£nh b√°o
            for tid, alert_type, metadata in self.person_tracker.check_alerts():
                if self.person_alert_callback:
                    alert_frame = full.copy()
                    self.draw_overlays(alert_frame, True, scale_x, scale_y)
                    self.person_alert_callback(self.source_id, alert_frame, alert_type, metadata)
                    
        except Exception as e:
            print(f"L·ªói x·ª≠ l√Ω ng∆∞·ªùi: {e}")
    
    # X·ª≠ l√Ω ph√°t hi·ªán t√© ng√£
    # Ch·ªâ ch·∫°y khi ƒë√£ ph√°t hi·ªán ƒë∆∞·ª£c ng∆∞·ªùi
    def process_fall(self, frame, scale_x, scale_y):
        if not self.fall_detector:
            return
        
        try:
            # C·∫≠p nh·∫≠t fall detector v·ªõi frame hi·ªán t·∫°i
            self.fall_detector.update(frame)
            
            # Ki·ªÉm tra tr·∫°ng th√°i t√© ng√£
            is_fall, prob = self.fall_detector.check_fall()
            self.is_fall_detected = is_fall
            self.fall_prob = prob
            
            # G·ª≠i c·∫£nh b√°o n·∫øu ph√°t hi·ªán t√© ng√£
            if is_fall and self.fall_alert_callback:
                alert_frame = frame.copy()
                self.draw_overlays(alert_frame, True, scale_x, scale_y)
                self.fall_alert_callback(self.source_id, alert_frame, AlertType.FALL)
                print(f"üö® FALL DETECTED - Camera {self.source_id} (prob={prob:.2f})")
                
                # Reset ƒë·ªÉ kh√¥ng g·ª≠i li√™n t·ª•c
                self.fall_detector.reset()
                
        except Exception as e:
            print(f"L·ªói ph√°t hi·ªán t√© ng√£: {e}")
    
    # X·ª≠ l√Ω k·∫øt qu·∫£ t·ª´ c√°c worker queue
    def process_results(self, frame, scale_x, scale_y):
        self.fire_boxes = []
        
        try:
            while not self.result_queue.empty():
                result = self.result_queue.get_nowait()
                result_type = result[0]
                
                if result_type == 'fire':
                    detections = result[1]
                    self.handle_fire_detections(detections, frame, scale_x, scale_y)
                    
        except queue.Empty:
            pass
    
    # X·ª≠ l√Ω ph√°t hi·ªán ch√°y v·ªõi h·ªá th·ªëng Red Alert Mode
    def handle_fire_detections(self, detections, frame, scale_x, scale_y):
        
        # Yellow Alert: Nghi ng·ªù c√≥ ch√°y (c·∫ßn x√°c nh·∫≠n th√™m)
        # Red Alert: Ch·∫Øc ch·∫Øn c√≥ ch√°y (nguy hi·ªÉm!)
        validated_dets = []
        
        for det in detections:
            bbox = det['bbox']
            
            # Validate v·ªõi b·ªô l·ªçc (lo·∫°i b·ªè false positive)
            if not self.fire_filter.validate(frame, bbox, self.is_ir):
                continue
            
            # Scale t·ªça ƒë·ªô v·ªÅ k√≠ch th∆∞·ªõc g·ªëc
            x1, y1, x2, y2 = bbox
            scaled_bbox = (
                int(x1 * scale_x), int(y1 * scale_y),
                int(x2 * scale_x), int(y2 * scale_y)
            )
            
            self.fire_boxes.append(scaled_bbox)
            self.fire_history.append({'time': time.time(), **det})
            validated_dets.append(det)
        
        # C·∫≠p nh·∫≠t fire tracker v√† ki·ªÉm tra ƒëi·ªÅu ki·ªán c·∫£nh b√°o
        now = time.time()
        should_alert, is_yellow, is_red = self.fire_tracker.update(validated_dets, now)
        
        # G·ª≠i c·∫£nh b√°o n·∫øu c·∫ßn
        if should_alert and self.fire_alert_callback:
            alert_frame = frame.copy()
            self.draw_overlays(alert_frame, True, scale_x, scale_y)
            
            # Red = CRITICAL, Yellow = WARNING
            alert_type = AlertType.FIRE_CRITICAL if is_red else AlertType.FIRE_WARNING
            
            if is_red:
                print(f"üî¥ RED ALERT - Camera {self.source_id}")
            elif is_yellow:
                print(f"üü° Yellow Alert - Camera {self.source_id}")
            
            self.fire_alert_callback(self.source_id, alert_frame, alert_type)
    
    # V·∫Ω c√°c th√¥ng tin l√™n frame
    def draw_overlays(self, frame, detection_enabled, scale_x=1.0, scale_y=1.0):
        
        # ----- V·∫Ω box ch√°y (ƒë·ªè) -----
        for box in self.fire_boxes:
            cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 0, 255), 3)
            
            # Label ch√≠nh
            cv2.putText(frame, "üî• FIRE", (box[0], box[1] - 5),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            
            # Hi·ªÉn th·ªã ƒë·ªô ph√°t tri·ªÉn (Growth)
            growth = self.fire_tracker.current_growth_rate
            if growth > 1.05: # Ch·ªâ hi·ªán khi tƒÉng > 5%
                text = f"Growth: +{(growth-1)*100:.0f}%"
                cv2.putText(frame, text, (box[0], box[1] - 25),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 165, 255), 2)
        
        # ----- V·∫Ω box ng∆∞·ªùi -----
        if detection_enabled:
            self.draw_persons(frame, scale_x, scale_y)
        
        # ----- V·∫Ω box chuy·ªÉn ƒë·ªông (Cyan) -----
        if hasattr(self.motion_detector, 'motion_boxes'):
            sx = scale_x
            sy = scale_y
            
            for (mx1, my1, mx2, my2) in self.motion_detector.motion_boxes:
                final_x1 = int(mx1 * sx)
                final_y1 = int(my1 * sy)
                final_x2 = int(mx2 * sx)
                final_y2 = int(my2 * sy)
                
                cv2.rectangle(frame, (final_x1, final_y1), (final_x2, final_y2), (255, 255, 0), 1)
                cv2.putText(frame, "Motion", (final_x1, final_y1 - 2), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 0), 1)

        # ----- Hi·ªÉn th·ªã ch·∫ø ƒë·ªô IR -----
        if self.is_ir:
            cv2.putText(frame, "IR MODE", (10, frame.shape[0] - 20),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
        
        # ----- Hi·ªÉn th·ªã ph√°t hi·ªán t√© ng√£ -----
        if self.is_fall_detected:
            # V·∫Ω ch·ªØ FALL DETECTED l·ªõn ·ªü gi·ªØa m√†n h√¨nh
            h, w = frame.shape[:2]
            text = "FALL DETECTED!"
            font = cv2.FONT_HERSHEY_SIMPLEX
            font_scale = 1.5
            thickness = 3
            (text_w, text_h), _ = cv2.getTextSize(text, font, font_scale, thickness)
            
            x = (w - text_w) // 2
            y = 60
            
            # V·∫Ω n·ªÅn ƒë·ªè
            cv2.rectangle(frame, (x - 10, y - text_h - 10), (x + text_w + 10, y + 10), (0, 0, 200), -1)
            cv2.putText(frame, text, (x, y), font, font_scale, (255, 255, 255), thickness)
            
            # Hi·ªÉn th·ªã x√°c su·∫•t
            prob_text = f"Prob: {self.fall_prob:.2f}"
            cv2.putText(frame, prob_text, (x, y + 35), font, 0.7, (0, 0, 255), 2)
    
    # V·∫Ω box ng∆∞·ªùi
    def draw_persons(self, frame, scale_x=1.0, scale_y=1.0):
        tracks = self.person_tracker.tracks
        
        for tid, track in tracks.items():
            x1, y1, x2, y2 = map(int, track.bbox)
            
            # X√°c ƒë·ªãnh t√™n hi·ªÉn th·ªã
            face_enabled = self.face_enabled if self.face_enabled is not None else settings.get('detection.face_recognition_enabled', True)
            
            # X√°c ƒë·ªãnh t√™n v√† tr·∫°ng th√°i
            if face_enabled:
                name = track.confirmed_name or track.name
                is_stranger = (name == "Stranger")
            else:
                name = "Person"
                is_stranger = False
            
            # ===== X√ÅC ƒê·ªäNH M√ÄU BOX =====
            if is_stranger:
                color = (0, 165, 255)    # Cam - Ng∆∞·ªùi l·∫°
            else:
                color = (0, 255, 0)      # Xanh l√° - Ng∆∞·ªùi quen
            
            # ===== V·∫º BOX =====
            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
            
            # ===== T·∫†O LABEL =====
            label = f"ID:{tid} {name}"
            
            # ===== V·∫º LABEL =====
            font = cv2.FONT_HERSHEY_SIMPLEX
            font_scale = 0.5
            thickness = 1
            (text_w, text_h), _ = cv2.getTextSize(label, font, font_scale, thickness)
            
            label_y1 = max(0, y1 - text_h - 10)
            label_y2 = y1 - 2
            label_x2 = min(frame.shape[1], x1 + text_w + 8)
            
            cv2.rectangle(frame, (x1, label_y1), (label_x2, label_y2), color, -1)
            cv2.putText(frame, label, (x1 + 4, label_y2 - 4), font, font_scale, (255, 255, 255), thickness)
    
    # Ph√°t hi·ªán ch·∫ø ƒë·ªô IR (h·ªìng ngo·∫°i/ban ƒë√™m)
    def detect_ir(self, frame):
        
        # Camera IR ch·ªâ c√≥ ƒëen tr·∫Øng, kh√¥ng c√≥ m√†u.
        # Khi camera chuy·ªÉn sang ban ƒë√™m, c·∫ßn ƒëi·ªÅu ch·ªânh c√°c ng∆∞·ª°ng.
        # N·∫øu user ƒë√£ toggle th·ªß c√¥ng, kh√¥ng auto detect
        if self.ir_manual_override is not None:
            return
        
        # L·∫•y m·∫´u (sample) ƒë·ªÉ t√≠nh nhanh
        sample = frame[::10, ::10]
        
        # T√°ch k√™nh m√†u
        b, g, r = cv2.split(sample.astype(np.float32))
        
        # T√≠nh trung b√¨nh v√† ƒë·ªô l·ªách chu·∫©n
        means = [np.mean(r), np.mean(g), np.mean(b)]
        std = np.std(means)
        ratio = min(means) / max(means) if max(means) > 0 else 1.0
        
        # T√≠nh ƒë·ªô b√£o h√≤a
        hsv = cv2.cvtColor(sample.astype(np.uint8), cv2.COLOR_BGR2HSV)
        sat = np.mean(hsv[:, :, 1])
        
        # IR: C√°c k√™nh m√†u g·∫ßn b·∫±ng nhau + ƒë·ªô b√£o h√≤a th·∫•p
        is_ir = std < 2.0 and ratio > 0.98 and sat < 10
        self.ir_history.append(is_ir)
        
        # C·∫ßn ƒë·ªß l·ªãch s·ª≠ ƒë·ªÉ quy·∫øt ƒë·ªãnh
        if len(self.ir_history) >= 10:
            ir_ratio = sum(self.ir_history) / len(self.ir_history)
            new_mode = ir_ratio >= 0.7
            
            # Th√¥ng b√°o khi chuy·ªÉn ch·∫ø ƒë·ªô
            if new_mode != self.is_ir:
                self.is_ir = new_mode
                mode_name = 'IR (Ban ƒë√™m)' if new_mode else 'RGB (Ban ng√†y)'
                print(f"üì∑ Camera {self.source_id}: Chuy·ªÉn sang ch·∫ø ƒë·ªô {mode_name}")
                if new_mode:
                    print(f"   ‚Üí T·∫Øt nh·∫≠n di·ªán khu√¥n m·∫∑t (·∫£nh ƒëen tr·∫Øng)")
    
    # √Åp d·ª•ng b·ªô l·ªçc m√†u theo ch·∫ø ƒë·ªô
    def apply_color_filter(self, frame):
        if self.is_ir:
            # Chuy·ªÉn sang grayscale ƒë·ªÉ x·ª≠ l√Ω th·ªëng nh·∫•t
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            return cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)
        
        # Tr·∫£ v·ªÅ frame g·ªëc
        return frame
    
    # Th·ª≠ k·∫øt n·ªëi l·∫°i camera
    def reconnect(self):
        self.reconnect_attempts += 1
        
        max_attempts = settings.get('camera.max_reconnect_attempts', 10)
        if self.reconnect_attempts > max_attempts:
            print(f"‚ùå Camera {self.source_id}: ƒê√£ th·ª≠ {max_attempts} l·∫ßn, d·ª´ng k·∫øt n·ªëi l·∫°i")
            return False
        
        print(f"ƒêang k·∫øt n·ªëi l·∫°i camera {self.source_id}... (l·∫ßn {self.reconnect_attempts}/{max_attempts})")
        
        if self.cap:
            self.cap.release()
        
        time.sleep(2.0)
        self.init_capture()
        
        if self.cap and self.cap.isOpened():
            self.reconnect_attempts = 0
            return True
        
        return False
    
    # Ki·ªÉm tra camera c√≤n ho·∫°t ƒë·ªông kh√¥ng
    def check_health(self):
        return time.time() - self.last_frame_time < 10
    
    # L·∫•y tr·∫°ng th√°i k·∫øt n·ªëi
    def get_connection_status(self):
        return self.cap is not None and self.cap.isOpened() and self.check_health()
    
    # Ki·ªÉm tra c√≥ m·ªëi nguy hi·ªÉm ƒëang ho·∫°t ƒë·ªông kh√¥ng
    # D√πng ƒë·ªÉ quy·∫øt ƒë·ªãnh c√≥ k√©o d√†i th·ªùi gian ghi video kh√¥ng
    # 1. Ki·ªÉm tra ch√°y
    def has_active_threat(self):
        if self.fire_tracker.get_is_red_alert() or self.fire_tracker.get_is_yellow_alert():
            return True
        
        # 2. Ki·ªÉm tra ng∆∞·ªùi l·∫°
        if self.person_tracker.has_active_threats():
            return True
        
        return False
    
    # L·∫•y tr·∫°ng th√°i IR
    def get_infrared_status(self):
        return self.is_ir
    
    # B·∫≠t/t·∫Øt ch·∫ø ƒë·ªô IR th·ªß c√¥ng (disable auto detect)
    def set_ir_enhancement(self, enabled):
        self.ir_manual_override = enabled  # ƒê√°nh d·∫•u ƒëang d√πng manual
        self.is_ir = enabled
        mode = "IR (Ban ƒë√™m)" if enabled else "RGB (Ban ng√†y)"
        print(f"üì∑ Camera {self.source_id}: Chuy·ªÉn sang ch·∫ø ƒë·ªô {mode} (manual)")
    
    # Reset v·ªÅ auto detect IR
    def reset_ir_auto(self):
        self.ir_manual_override = None
        self.ir_history.clear()
        print(f"üì∑ Camera {self.source_id}: Reset v·ªÅ auto detect IR")
    
    # B·∫Øt bu·ªôc k·∫øt n·ªëi l·∫°i
    def force_reconnect(self):
        self.reconnect_attempts = 0
        self.reconnect()
    
    # Gi·∫£i ph√≥ng t√†i nguy√™n
    def release(self):
        self.quit = True
        
        # Gi·∫£i ph√≥ng fall detector
        if self.fall_detector:
            self.fall_detector.close()
            self.fall_detector = None
        
        if self.cap:
            self.cap.release()
            self.cap = None
